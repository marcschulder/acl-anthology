<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.tacl">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 8</booktitle>
      <year>2020</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <title>Phonotactic Complexity and Its Trade-offs</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <doi>10.1162/tacl_a_00296</doi>
      <abstract>We present methods for calculating a measure of phonotactic complexity—bits per phoneme— that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language’s phonotactics is. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of − 0.74 between bits per phoneme and the average length of words.</abstract>
      <pages>1–18</pages>
      <url hash="c153b336">2020.tacl-1.1</url>
    </paper>
    <paper id="2">
      <title><fixed-case>AMR</fixed-case>-To-Text Generation with Graph Transformer</title>
      <author><first>Tianming</first><last>Wang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Hanqi</first><last>Jin</last></author>
      <doi>10.1162/tacl_a_00297</doi>
      <abstract>Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. The current state-of-the-art methods use graph-to-sequence models; however, they still cannot significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The model directly encodes the AMR graphs and learns the node representations. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively. Our model outperforms the state-of-the-art neural approach by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.</abstract>
      <pages>19–33</pages>
      <url hash="9dce31b7">2020.tacl-1.2</url>
    </paper>
    <paper id="3">
      <title>What <fixed-case>BERT</fixed-case> Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models</title>
      <author><first>Allyson</first><last>Ettinger</last></author>
      <doi>10.1162/tacl_a_00298</doi>
      <abstract>Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.</abstract>
      <pages>34–48</pages>
      <url hash="50cfa1c2">2020.tacl-1.3</url>
    </paper>
    <paper id="4">
      <title>Membership Inference Attacks on Sequence-to-Sequence Models: <fixed-case>I</fixed-case>s My Data In Your Machine Translation System?</title>
      <author><first>Sorami</first><last>Hisamoto</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <doi>10.1162/tacl_a_00299</doi>
      <abstract>Data privacy is an important issue for “machine learning as a service” providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model’s API, determine whether the sample existed in the model’s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.</abstract>
      <pages>49–63</pages>
      <url hash="dd95e987">2020.tacl-1.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>pan<fixed-case>BERT</fixed-case>: Improving Pre-training by Representing and Predicting Spans</title>
      <author><first>Mandar</first><last>Joshi</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <author><first>Yinhan</first><last>Liu</last></author>
      <author><first>Daniel S.</first><last>Weld</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <doi>10.1162/tacl_a_00300</doi>
      <abstract>We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1</abstract>
      <pages>64–77</pages>
      <url hash="27aaf55c">2020.tacl-1.5</url>
    </paper>
    <paper id="6">
      <title>A Graph-based Model for Joint <fixed-case>C</fixed-case>hinese Word Segmentation and Dependency Parsing</title>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <doi>10.1162/tacl_a_00301</doi>
      <abstract>Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined at the word-level. Therefore word segmentation is the precondition of dependency parsing, which makes dependency parsing suffer from error propagation and unable to directly make use of character-level pre-trained language models (such as BERT). In this paper, we propose a graph-based model to integrate Chinese word segmentation and dependency parsing. Different from previous transition-based joint models, our proposed model is more concise, which results in fewer efforts of feature engineering. Our graph-based joint model achieves better performance than previous joint models and state-of-the-art results in both Chinese word segmentation and dependency parsing. Additionally, when BERT is combined, our model can substantially reduce the performance gap of dependency parsing between joint models and gold-segmented word-based models. Our code is publicly available at https://github.com/fastnlp/JointCwsParser</abstract>
      <pages>78–92</pages>
      <url hash="8c82b722">2020.tacl-1.6</url>
    </paper>
    <paper id="7">
      <title>A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Zhihao</first><last>Zhao</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <doi>10.1162/tacl_a_00302</doi>
      <abstract>Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.</abstract>
      <pages>93–108</pages>
      <url hash="c075234a">2020.tacl-1.7</url>
    </paper>
    <paper id="8">
      <title>Improving Candidate Generation for Low-resource Cross-lingual Entity Linking</title>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <doi>10.1162/tacl_a_00303</doi>
      <abstract>Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL.1</abstract>
      <pages>109–124</pages>
      <url hash="74d04e8d">2020.tacl-1.8</url>
    </paper>
    <paper id="9">
      <title>Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks</title>
      <author><first>R. Thomas</first><last>McCoy</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <doi>10.1162/tacl_a_00304</doi>
      <abstract>Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.</abstract>
      <pages>125–140</pages>
      <url hash="2d0d4357">2020.tacl-1.9</url>
    </paper>
    <paper id="10">
      <title>Investigating Prior Knowledge for Challenging <fixed-case>C</fixed-case>hinese Machine Reading Comprehension</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <doi>10.1162/tacl_a_00305</doi>
      <abstract>Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations. We present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5%) and human readers (96.0%), especiallyon problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C3 to present great challenges to existing systems as answering 86.8% of questions requires both knowledge within and beyond the accompanying document, and we hope that C3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text. C3 is available at https://dataset.org/c3/.</abstract>
      <pages>141–155</pages>
      <url hash="2e13d3fe">2020.tacl-1.10</url>
    </paper>
    <paper id="11">
      <title>Theoretical Limitations of Self-Attention in Neural Sequence Models</title>
      <author><first>Michael</first><last>Hahn</last></author>
      <doi>10.1162/tacl_a_00306</doi>
      <abstract>Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.</abstract>
      <pages>156–171</pages>
      <url hash="7be349b5">2020.tacl-1.11</url>
    </paper>
    <paper id="12">
      <title>Target-Guided Structured Attention Network for Target-Dependent Sentiment Analysis</title>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Chengyao</first><last>Chen</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Chao</first><last>He</last></author>
      <author><first>Cane Wing-Ki</first><last>Leung</last></author>
      <doi>10.1162/tacl_a_00308</doi>
      <abstract>Target-dependent sentiment analysis (TDSA) aims to classify the sentiment of a text towards a given target. The major challenge of this task lies in modeling the semantic relatedness between a target and its context sentence. This paper proposes a novel Target-Guided Structured Attention Network (TG-SAN), which captures target-related contexts for TDSA in a fine-to-coarse manner. Given a target and its context sentence, the proposed TG-SAN first identifies multiple semantic segments from the sentence using a target-guided structured attention mechanism. It then fuses the extracted segments based on their relatedness with the target for sentiment classification. We present comprehensive comparative experiments on three benchmarks with three major findings. First, TG-SAN outperforms the state-of-the-art by up to 1.61% and 3.58% in terms of accuracy and Marco-F1, respectively. Second, it shows a strong advantage in determining the sentiment of a target when the context sentence contains multiple semantic segments. Lastly, visualization results show that the attention scores produced by TG-SAN are highly interpretable</abstract>
      <pages>172–182</pages>
      <url hash="1e1d55c3">2020.tacl-1.12</url>
    </paper>
    <paper id="13">
      <title>Break It Down: A Question Understanding Benchmark</title>
      <author><first>Tomer</first><last>Wolfson</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Ankit</first><last>Gupta</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Daniel</first><last>Deutch</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <doi>10.1162/tacl_a_00309</doi>
      <abstract>Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.</abstract>
      <pages>183–198</pages>
      <url hash="08ba3326">2020.tacl-1.13</url>
    </paper>
    <paper id="14">
      <title>Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies</title>
      <author><first>Xi (Leslie)</first><last>Chen</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Michelle</first><last>Levine</last></author>
      <author><first>Marko</first><last>Mandic</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <doi>10.1162/tacl_a_00311</doi>
      <abstract>Humans rarely perform better than chance at lie detection. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built classifiers to automatically distinguish trusted from mistrusted speech, achieving an F1 of 66.1%. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the strategies that judges reported using in deception detection were not helpful for the task. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.</abstract>
      <pages>199–214</pages>
      <url hash="d0dfd22b">2020.tacl-1.14</url>
    </paper>
    <paper id="15">
      <title>Unsupervised Discourse Constituency Parsing Using <fixed-case>V</fixed-case>iterbi <fixed-case>EM</fixed-case></title>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <doi>10.1162/tacl_a_00312</doi>
      <abstract>In this paper, we introduce an unsupervised discourse constituency parsing algorithm. We use Viterbi EM with a margin-based criterion to train a span-based discourse parser in an unsupervised manner. We also propose initialization methods for Viterbi training of discourse constituents based on our prior knowledge of text structures. Experimental results demonstrate that our unsupervised parser achieves comparable or even superior performance to fully supervised parsers. We also investigate discourse constituents that are learned by our method.</abstract>
      <pages>215–230</pages>
      <url hash="9b0c0560">2020.tacl-1.15</url>
    </paper>
  </volume>
</collection>
